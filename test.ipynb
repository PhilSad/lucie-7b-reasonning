{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e2b790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<think>.*?</think>\\s*<answer>.*?</answer>$\"\n",
    "    completion_contents = completions\n",
    "    matches = [re.match(pattern, content) for content in completion_contents]\n",
    "    \n",
    "    \n",
    "    rewards_full_list = [1.0 if match else 0.0 for match in matches]\n",
    "    for i, completion in enumerate(completion_contents):\n",
    "        if not completion.startswith(\"<think>\") or not completion.endswith(\"</answer>\"):\n",
    "            rewards_full_list[i] = 0.0\n",
    "    print(f\"Rewards for completions: {rewards_full_list}\")\n",
    "    \n",
    "    rewards_partial_1 = [1.0 if content.count(\"<think>\") == 1 else 0.0 for content in completion_contents]\n",
    "    rewards_partial_2 = [1.0 if content.count(\"</think>\") == 1 else 0.0 for content in completion_contents]\n",
    "    rewards_partial_3 = [1.0 if content.count(\"<answer>\") == 1 else 0.0 for content in completion_contents]\n",
    "    rewards_partial_4 = [1.0 if content.count(\"</answer>\") == 1 else 0.0 for content in completion_contents]\n",
    "    \n",
    "    # Combine the rewards\n",
    "    final_rewards = []\n",
    "    for full, p1, p2, p3, p4 in zip(rewards_full_list, rewards_partial_1, rewards_partial_2, rewards_partial_3, rewards_partial_4):\n",
    "        final_reward = (full + (p1 + p2 + p3 + p4) / 4.0) / 2.0\n",
    "        final_rewards.append(final_reward)\n",
    "    \n",
    "    return rewards_full_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0da1ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from rouge_score import rouge_scorer\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<think>.*?</think>\\s*<answer>.*?</answer>$\"\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, content) for content in completion_contents]\n",
    "    \n",
    "    \n",
    "    rewards_full_list = [1.0 if match else 0.0 for match in matches]\n",
    "    for i, completion in enumerate(completion_contents):\n",
    "        if not completion.startswith(\"<think>\") or not completion.endswith(\"</answer>\"):\n",
    "            rewards_full_list[i] = 0.0\n",
    "    \n",
    "    rewards_partial_1 = [1.0 if content.count(\"<think>\") == 1 else 0.0 for content in completion_contents]\n",
    "    rewards_partial_2 = [1.0 if content.count(\"</think>\") == 1 else 0.0 for content in completion_contents]\n",
    "    rewards_partial_3 = [1.0 if content.count(\"<answer>\") == 1 else 0.0 for content in completion_contents]\n",
    "    rewards_partial_4 = [1.0 if content.count(\"</answer>\") == 1 else 0.0 for content in completion_contents]\n",
    "    \n",
    "    # Combine the rewards\n",
    "    final_rewards = []\n",
    "    for full, p1, p2, p3, p4 in zip(rewards_full_list, rewards_partial_1, rewards_partial_2, rewards_partial_3, rewards_partial_4):\n",
    "        final_reward = (full + (p1 + p2 + p3 + p4) / 4.0) / 2.0\n",
    "        final_rewards.append(final_reward)\n",
    "    \n",
    "    return rewards_full_list\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "import evaluate\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "def accuracy_reward(completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion is the same as the ground truth.\"\"\"\n",
    "    solutions = kwargs[\"solution\"]\n",
    "    think = kwargs[\"think\"]\n",
    "    completion_contents = completions\n",
    "    \n",
    "    # answer\n",
    "    completion_filt = []\n",
    "    for completion in completion_contents:\n",
    "        if \"<answer>\" in completion:\n",
    "            completion_filt.append(completion[completion.index(\"<answer>\") + len(\"<answer>\"):])\n",
    "        elif \"</think>\" in completion:\n",
    "            completion_filt.append(completion[completion.index(\"</think>\") + len(\"</think>\"):])\n",
    "        else:\n",
    "            completion_filt.append(completion)\n",
    "    \n",
    "    results = bertscore.compute(predictions=completion_filt, \n",
    "                            references=solutions, \n",
    "                            lang=\"en\",\n",
    "                            device=\"cuda\")\n",
    "    scores_answer = results['f1']\n",
    "    \n",
    "    # think\n",
    "    completion_filt = []\n",
    "    for completion in completion_contents:\n",
    "        if \"</think>\" in completion and \"<think>\" in completion:\n",
    "            think_start = completion.index(\"<think>\") + len(\"<think>\")\n",
    "            think_end = completion.index(\"</think>\")\n",
    "            completion_filt.append(completion[think_start:think_end])\n",
    "        else:\n",
    "            completion_filt.append(completion)\n",
    "    results = bertscore.compute(predictions=completion_filt, \n",
    "                            references=think, \n",
    "                            lang=\"en\",\n",
    "                            device=\"cuda\")\n",
    "    scores_think = results['f1']\n",
    "    scores = []\n",
    "    for score_answer, score_think in zip(scores_answer, scores_think):\n",
    "        scores.append((score_answer + score_think) / 2.0)\n",
    "    \n",
    "    return scores\n",
    "import numpy as np\n",
    "def long_think_reward(completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion has a long think process and / short answer\"\"\"\n",
    "    \n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        content = completion[0][\"content\"]\n",
    "        think_start = content.find(\"<think>\") + len(\"<think>\")\n",
    "        think_end = content.find(\"</think>\")\n",
    "        answer_start = content.find(\"<answer>\") + len(\"<answer>\")\n",
    "        answer_end = content.find(\"</answer>\")\n",
    "        \n",
    "        if think_start == -1 or think_end == -1 or answer_start == -1 or answer_end == -1:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        think_length = think_end - think_start\n",
    "        answer_length = answer_end - answer_start\n",
    "        \n",
    "        W_think = 1\n",
    "        W_answer = 1\n",
    "        reward = (W_think * think_length - W_answer * answer_length) \n",
    "        # sigmoid function to normalize the reward\n",
    "        k = 0.01\n",
    "        reward = 1 / (1 + np.exp(-k * reward))\n",
    "        scores.append(reward)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "898389b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards for completions: [0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completions = [\n",
    "    \"fmlskjfksjf\\n<think> This is a thought process </think> <answer> This is the answer </answer>\",\n",
    "]\n",
    "# Example usage\n",
    "format_reward(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95c386b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "import os\n",
    "model = \"gemini-2.0-flash-lite\"\n",
    "client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82116d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_evaluate(question, reference_answer, answer_to_be_judged):\n",
    "    prompt_template = \"\"\"You are an expert evaluator. Your task is to rate the following model-generated answer for correctness on a scale of 0.0 to 1.0.\n",
    "\n",
    "    **Question:**\n",
    "    {the_question}\n",
    "\n",
    "    **Ground-Truth Solution:**\n",
    "    {the_reference_answer}\n",
    "\n",
    "    **Model's Generated Answer:**\n",
    "    {the_answer_to_be_judged}\n",
    "\n",
    "    **Evaluation Criteria:**\n",
    "    evaluate the factual accuracy in the context of the question.\n",
    "\n",
    "    Based on these criteria, provide a single floating-point score from 0.0 (completely wrong) to 1.0 (perfectly correct).\n",
    "\n",
    "    **Score:**\"\"\"\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        the_question=question,\n",
    "        the_reference_answer=reference_answer,\n",
    "        the_answer_to_be_judged=answer_to_be_judged\n",
    "    )\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=[prompt],\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        score = float(response.text)\n",
    "    except ValueError:\n",
    "        print(f\"Error parsing score from response: {response.text}\")\n",
    "        score = 0.5\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9812bc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
